import nltk.translate.meteor_score as scorer
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as nnf

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


def compute_score(references, hypothesis_sampled, hypothesis_greedy):
    meteors = []
    for idx in range(len(hypothesis_sampled)):
        meteors.append(scorer.single_meteor_score(references[idx], hypothesis_sampled[idx]))

    meteors_greedy = [scorer.single_meteor_score(references[0], hypothesis_greedy[0])] * len(hypothesis_greedy)
    meteors.extend(meteors_greedy)

    return meteors


def get_self_critical_reward(res_sample, res_greedy, gts, max_len=20):
    batch_size = len(res_sample)

    # gts = dict(enumerate(gts + gts))
    # res = [{'image_id': i, 'caption': r} for i, r in enumerate(list(res_sample.values()) + list(res_greedy.values()))]

    # batch_size * 2 len
    scores = np.asarray(compute_score(gts, res_sample, res_greedy))
    # r(w) - r(w_hat) = batch_size len
    scores = scores[:batch_size] - scores[batch_size:]

    # repeat same reward for all timesteps
    reward = np.repeat(scores[:, np.newaxis], max_len, 1)
    reward = torch.from_numpy(reward).float().to(device)

    return reward, scores.mean()


class SelfCriticalLoss(nn.Module):
    """
    Reinforcement Learning Self-critical loss.
    https://arxiv.org/abs/1612.00563
    Code from https://github.com/ruotianluo/self-critical.pytorch
    """

    def __init__(self, vocab):
        super(SelfCriticalLoss, self).__init__()
        self.vocab = vocab

    def forward(self, sample, sample_log_probs, greedy_sample, gts_batch, lengths, return_advantage=False):
        assert len(sample) == len(sample_log_probs) == len(greedy_sample) == len(gts_batch) == len(lengths)

        max_len_batch = max([sum(_) for _ in lengths])
        # Don't look for starting token because is always first given and then discarded, during sampling
        res_sample, res_greedy, gts = self.get_caps(greedy_sample, sample, gts_batch)
        reward, advantage = get_self_critical_reward(res_sample, res_greedy, gts, max_len=max_len_batch)

        # Mask tokens out after <end>, and tokens not generated by the network
        mask = self.create_mask(sample, max_len_batch)

        log_probs = []
        for idx in range(len(sample_log_probs)):
            log_prob = sample_log_probs[idx].gather(1, sample[idx].unsqueeze(1).to(device)).view(-1)
            log_probs.append(nnf.pad(log_prob,
                                     pad=[0, max_len_batch - log_prob.shape[0]],
                                     mode='constant', value=0.0))

        log_probs = torch.stack(log_probs)

        loss = - (log_probs * reward * mask).sum() / mask.sum()

        return (loss, advantage) if return_advantage else loss

    def convert_to_sentences(self, idx_form):
        word_form = []
        for idx, sentence in enumerate(idx_form):
            words = []
            jdx = 0
            while jdx < sentence.shape[0]:
                word_id = sentence[jdx].cpu().item()
                word = self.vocab.idx2word[word_id]
                if word == '<start>':
                    jdx += 1
                elif word == '<end>':
                    while jdx < sentence.shape[0] and sentence[jdx].cpu().item() != 1:
                        jdx += 1
                else:
                    words.append(word)
                    jdx += 1

            word_form.append(' '.join(words))

        return word_form

    def get_caps(self, greedy_sample, sample, gts):
        res_greedy = self.convert_to_sentences(greedy_sample)
        res_sample = self.convert_to_sentences(sample)

        _gts = self.convert_to_sentences(gts)

        return res_sample, res_greedy, _gts

    def create_mask(self, sample, sample_max_len):
        """
        Mask tokens out if they're forced. I.e. when start of sentence token is always given instead of predicted,
        so no loss would be needed for it.
        Mask also tokens after <end>, because they shouldn't get any gradient.
        :param sample:
        :param sample_max_len:
        :return: mask
        """
        num_samples = len(sample)

        mask = torch.zeros([num_samples, sample_max_len]).float().to(device)
        end_idx = [sample[i].shape[0] for i in range(num_samples)]

        for i, ei in enumerate(end_idx):
            mask[i][:ei] = 1

        return mask


if __name__ == '__main__':
    print('Testing METEOR...\n')
    gts = 'the cat sat on the mat'
    print(f'Ground truth: "{gts}"\n')
    hyp = 'on the mat sat the cat'

    print(f'Score for: "{hyp}" = {scorer.single_meteor_score(gts, hyp)}')

    hyp = 'the crow sat on the mat'

    print(f'Score for: "{hyp}" = {scorer.single_meteor_score(gts, hyp)}')

    hyp = 'the cat sat on the mat'

    print(f'Score for: "{hyp}" = {scorer.single_meteor_score(gts, hyp)}')

    print('\n...all tests passed!')
